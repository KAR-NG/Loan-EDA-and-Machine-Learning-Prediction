---
title: "Loan EDA and Machine Learning Prediction"
author: "Kar Ng"
date: "1/24/2022"
output: 
  github_document: 
    toc: true
    toc_depth: 4
always_allow_html: yes
---


***

![](https://raw.githubusercontent.com/KAR-NG/loan/main/thumbnail.jpg)

***


## 1 SUMMARY

This project studies the effects of various loan-related variables and their effect on loan approval. These variables include gender, marital status, number of dependents, education level, employment status, applicant's income, co-applicant's income, loan amount, loan amount term, credit history, and property area. In feature engineering, three new variables were synthesised. They are total income (combination of applicant and co-applicant incomes), loan per month (loan amount / term), and whether the number of income providers in an application. 

Exploratory data analysis shown that credit history is the most important variable. It found that 80% of approved applicants had credit history compared to 20% approved applicants did not have credit history. Some other minor trends included that (1) there are more male applicants than female but both gender have equal acceptance rate, (2) higher education level may help a little with a 8% more in approved application, (3) a married relationship may also help a little with also a 9% more in approved application, and (4) if an application has two incomes (from both applicant + co-applicant), the application may get a higher chance to be approved (40% more approved applications had dual-income).  

Many models were built to search for the best model to make prediction on an unknown dataset. This project built a logistic regression model and many tree-based models included decision tree, bagging, random forest, and tuned extreme gradient boosting models. There was data unbalance problem in the dataset, and "Both-sampling" was applied to under-sample "Y" and over-sample "N" in the responding variable "Loan-Status" to solve the data unbalance problem.

In statistical modeling, random forest model with ROC-suggested probability "Y" cutoff point at 0.523 outperformed all other models. The random forest model has an accuracy rate of 77.4%, a "No Information rate" lower than the minimum point of 95% CI, and the model had the smallest gap between sensitivity and specificity, both at 80.5% and 70.3%. Random forest Important plots suggests that positive credit history is most important variable (supporting EDA), followed by applicant income, total income, loan amount per term, and credit history with level "Not sure". 

The random forest model with 0.523 probability "Y" cutoff point was applied to prediction loan application outcome on an unknown dataset。 The model predicted that 63% of application should be "Y" (at 80.5% of chance) and 37% of application should be "N" (at 70.3% of chance).

**Insights**

![](https://raw.githubusercontent.com/KAR-NG/loan/main/Highlight.png)


## 2 R PACKAGES

Following R packages are loaded. 

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(skimr)
library(caret)
library(kableExtra)
library(glmnet)
library(MASS)
library(pROC)
library(rpart)
library(rpart.plot)
library(rattle)   # plot tree from caret
library(ggrepel)
library(ROSE)

```

## 3 INTRODUCTION


This project uses datasets from [Kaggle.com](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset) that are related to loan application. *Kaggle.com* is a popular website for data science community. 

This project will study variables that are potentially related to loan application such as gender, marital status, number of dependents, education level, self employment, applicant income, co-applicant income, loan amount, loan amount term, credit history, and area of respondents' property. 

Exploratory data analysis will be applied to study the general trends between these variables with the outcome variable "loan status". Statistical modeling will also be applied to study the data and to extract statistical important variables. Many models will be built and the best model will be used to predict application outcome of an unknown dataset. 

Two datasets are given, one is named 'train.data' and the another named 'test.data'. Both datasets have same information but only the train.data has application outcome variable named 'loan status', which contains the loan results with either "Y" or "N". 

During modeling, I will split the "train.data" into two smaller datasets, one is named **"train.set"** and another named **"test.set"**. The **train.set** will be used to make various models and the **test.set** will be used to evaluate these models. Finally, the best model will be applied to predict application outcome of the test.data dataset.



## 4 DATA PREPARATION

Data is downloaded from the [Kaggle](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset). Two datasets are downloaded, a train.data and a test.data dataset.

### 4.1 Data import



```{r}
train.data <- read.csv("train_loan.csv", na.strings = c("", "NA"))
test.data <- read.csv("test_loan.csv", na.strings = c("", "NA"))

```

### 4.2 Data exploration

There are 13 variables for the train.data and 12 variables for the test.data. Both dataset have similar variable, but as mentioned, only the train.data has the application outcome "Loan_Status". The test.data is the dataset that requiring me to make application outcome prediction.

Following is the first 6 rows of the train.data and the test.data.

```{r}
head(train.data)

```

```{r}
head(test.data)
     
```
**Structural Analysis**

Train.data dataset:

```{r}
str(train.data)

```
Test.data dataset:

```{r}
str(test.data)

```
* There are 614 observations (rows) in the train.data dataset and 367 observations in the test.data dataset.      
* The last variable 'Loan_status' of train.data dataset has two levels either 'Y' or 'N' to indicate application outcome of an applicant.    
* The 'Loan_Amount_Term' is the length of time a loan is to be completely paid off. This variable has repeated levels. For example, there are many 360. It may indicate that this variable has categorical nature and should be converted in factor type. 

**NA Analysis**

To detect missing values (NA), 

* In the train.data,  
  * There are 8 character and 5 numerical variables.
  * The dataset is quite complete (checking on the **n_missing** and **complete_rate**).   
  * There are many variables with a few missing values, and they have complete_rate higher 90%.  
  * The variable **Credit_History** is either 1 or 0, which is a binary variable and should be converted into factor during data cleaning. 
  
```{r}
skim_without_charts(train.data)

```
* The same situation goes to the test.data dataset. 

```{r}
skim_without_charts(test.data)

```
## 5 DATA CLEANING AND MANIPULATION

### 5.1 Train.data 

#### 5.1.1 Removing ID and factor conversion

First, I will remove the "Loan_ID" and convert all character variables into factor because they are categorical variables. LOan_ID will be removed because it adds no value for prediction.

Converting variables into factors will help the analysis because it gives the data a grouping features which will enable useful R functions and useful visualisation in later stage. 

```{r}

train.data <- train.data %>% 
  dplyr::select(-Loan_ID) %>%                                    # Remove Loan_ID 
  mutate_if(is_character, as.factor) %>%                  # Convert all character into factor.
  mutate(Credit_History = as.factor(Credit_History),      # Convert Credit_History into factor.
         Loan_Amount_Term = as.factor(Loan_Amount_Term))  # Convert Loan_Amount_Term into factor.

```

After converting categorical variables into factor, following function help to summarise the dataset.  

```{r}

summary(train.data)

```
Many variables have missing values, as denoted by "NA's". Missing values will be filled up in next section.


#### 5.1.2 Replacing NA

Following codes replace all the NAs with respective mode. "Mode" is the most occurring category of a variable. It is a decision made after carefully examining its feasibility, these missing values are less than 5%, and their respective most occurring category have frequencies that are way higher than the rest of the categories. Therefore, it is highly likely that these NAs may belong to the most occurring category.

```{r}
train.data <- train.data %>% 
  mutate(Gender = replace_na(Gender, "Male"),
         Married = replace_na(Married, "Yes"),
         Dependents = replace_na(Dependents, '0'),
         Self_Employed = replace_na(Self_Employed, 'No'),
         Loan_Amount_Term = replace_na(Loan_Amount_Term, '360'))
  
summary(train.data)

```

**Credit History**

Regarding the NA in the variable "credit history", I found that the 50 NA is too many (8%) and is quite close to the 89 observations of the category "0" (mean no credit history). I decided to make a new level named "Not sure" to these "Na"

```{r}
50/(89+475+50) 

```
```{r}
train.data <- train.data %>% 
  mutate(Credit_History = as.character(Credit_History),
         Credit_History = replace_na(Credit_History, "Not_sure"),
         Credit_History = as.factor(Credit_History))
```

Now, the variable "Credit History" has following category (or known as "level") and respective counts. 

```{r}
summary(train.data$Credit_History)
```

**LoanAmount**

For the variable "LoanAmount", there are 22 missing values out of 614 rows of data (3.5%). I will remove these missing values. According to Schafer (1999), the paper asserted that a missing rate of 5% or less is inconsequential.

```{r}
22/614 * 100
```
```{r}
train.data <- train.data %>% na.omit()

```

### 5.2 Test data 

#### 5.1.1 Remove ID and factor conversion

Applying the same data processing techniques in the train.data dataset to test.data dataset. 

```{r}

test.data <- test.data %>% 
  dplyr::select(-Loan_ID) %>%                                    # Remove Loan_ID 
  mutate_if(is_character, as.factor) %>%                  # Convert all character into factor.
  mutate(Credit_History = as.factor(Credit_History),      # Convert Credit_History into factor.
         Loan_Amount_Term = as.factor(Loan_Amount_Term))  # Convert Loan_Amount_Term into factor.

summary(test.data)

```
#### 5.1.2 Replacing NA

```{r}
test.data <- test.data %>% 
  mutate(Gender = replace_na(Gender, "Male"),
         Dependents = replace_na(Dependents, '0'),
         Self_Employed = replace_na(Self_Employed, 'No'),
         Loan_Amount_Term = replace_na(Loan_Amount_Term, '360'))
  
summary(test.data)

```

**Credit History**

For credit history, I will transfer 29 of "NA" to "1", because it is very likely that all these 29 NA are actually 1  because 1 is the most occurring category of that variable and outnumbering "0".  

```{r}
test.data <- test.data %>% 
  mutate(Credit_History = as.character(Credit_History),
         Credit_History = replace_na(Credit_History, "1"),
         Credit_History = as.factor(Credit_History))

summary(test.data)

```

**Loan Amount**

There are only 5 NA in the variable, I will remove these missing values. 

```{r}
test.data <- test.data %>% 
  na.omit()

```


### 5.3 Feature Engineering

**On Train.data**

This section creates 3 new variables based on original variables. 

1. total_income (Applicant Income + Co-applicant income)

This is synthesised by adding "ApplicantIncome" and "CoapplicantIncome" to find the total income of an application. 

```{r}
train.data <- train.data %>% 
  mutate(total_income = ApplicantIncome + CoapplicantIncome) %>% 
  relocate(total_income, .after = CoapplicantIncome)

```

2. Loan_Amt_per_term (Loan/month)

This is synthesised by dividing "LoanAmount" with "Loan_Amount_Term" to find out would the loan amount with the unit of per month would affect the approval of a loan application. 

```{r}
train.data <- train.data %>% 
  mutate(Loan_Amt_per_term = round(LoanAmount/as.numeric(Loan_Amount_Term), 2)) %>% 
  relocate(Loan_Amt_per_term, .after = Loan_Amount_Term)

```


3. Income provider (one incomer or two)

This variable is created based on the numbers of incomers in an application. Two levels have been detected for this new variable:

* "Applicant": Only the primary applicant has income

* "Both": Both the primary applicant and co-applicant have incomes

```{r}
train.data <- train.data %>% 
  mutate(Income_provider = case_when(CoapplicantIncome == 0 ~ "Applicant",
                                     ApplicantIncome == 0 ~ "Co-applicant",
                                     TRUE ~ "Both"),
         Income_provider = as.factor(Income_provider)) %>% 
  relocate(Income_provider, .after = total_income)

```

**On Test.data**

The same operation applies to test.data.


1. total_income

```{r}
test.data <- test.data %>% 
  mutate(total_income = ApplicantIncome + CoapplicantIncome) %>% 
  relocate(total_income, .after = CoapplicantIncome)

```

2. Loan_Amt_per_term

```{r}
test.data <- test.data %>% 
  mutate(Loan_Amt_per_term = round(LoanAmount/as.numeric(Loan_Amount_Term), 2)) %>% 
  relocate(Loan_Amt_per_term, .after = Loan_Amount_Term)

```


3. Income provider (one incomer or two)

```{r}
test.data <- test.data %>% 
  mutate(Income_provider = case_when(CoapplicantIncome == 0 ~ "Applicant",
                                     ApplicantIncome == 0 ~ "Co-applicant",
                                     TRUE ~ "Both"),
         Income_provider = as.factor(Income_provider)) %>% 
  relocate(Income_provider, .after = total_income)

```


## 6 EXPLORATORY DATA ANALYSIS (EDA)

In this section, I will use the **train.data** dataset to analyse general trends in the dataset because **train.data** has the outcome variable "Loan_Status". The relationship between Loan_Status with all variables in the dataset will be investigated.

```{r}
names(train.data)

```
This exploratory data analysis will help to understand the general trends in the data before getting into machine learning.

### 6.1 Overall Approval Rate 

Most of the applications from the train dataset were approved. 411 applications (70%) were approved, denoted by the symbol "Y"， whereas 181 of applications (30%) were rejected, denoted by "N".

```{r, message=FALSE, warning=FALSE}

# set up dataframe

df6.1 <- train.data %>% 
  dplyr::select(Loan_Status) %>% 
  group_by(Loan_Status) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(total = sum(count),
         per = round(count/total, 1))

# plot

ggplot(df6.1, aes(x = "", y = count, fill = Loan_Status)) +
  geom_bar(stat = 'identity', colour = "black") +
  coord_polar(theta = "y", start = 0) +
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_label(aes(label = paste0(Loan_Status, "\n", count, " (", per*100, " %)")),
             position = position_stack(vjust = 0.5),
             size = 4) +
  labs(title = "70% Of Applications have been Approved",
       subtitle = "(Train data)")  

```

It is an unbalanced dataset because the number of observations in both categories are different from each other in a large scale, and which may cause problem in machine learning. In section 7.2, a technique will be used to address this data unbalanced problem. 

### 6.2 Would Gender affect Loan Approval

Results show that:

* There are lesser female applicants (109) than male applicants (483).

* However, the loan success rate of both gender are the same, both arrived at 70%. 

```{r, fig.width = 8, message=FALSE}

# set up dataframe

df6.2 <- train.data %>% 
  dplyr::select(Loan_Status, Gender) %>% 
  group_by(Loan_Status, Gender) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  group_by(Gender) %>% 
  mutate(total = sum(count),
         per = round(count/total, 1))

# plot

ggplot(df6.2, aes(x = "", y = count, fill = Loan_Status)) +
  geom_bar(stat = 'identity', colour = "black") +
  coord_polar(theta = "y", start = 0) +
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_label(aes(label = paste0(Loan_Status, "\n", count, " (", per*100, " %)")),
             position = position_stack(vjust = 0.5),
             size = 3) +
  labs(title = "Both Gender have similar proportion",
       subtitle = "(Train data)") +
  facet_wrap(~ Gender)


```
Female Applicants:

```{r}
73+36
```
Male Applicants:

```{r}
145+338

```

### 6.3 Income and Loan Approval

There are three type of income categories to be analysed, which are:

* "ApplicantIncome" - the income of the primary applicant  
* "CoapplicantIncome" - the income of co-applicant  
* "total_income" - the total income of both primary applicant and co-applicant  

```{r, fig.width=8, fig.height=7, message=FALSE, warning=FALSE}

# set up dataframe

df6.3 <- train.data %>% 
  dplyr::select(ApplicantIncome, CoapplicantIncome, total_income, Loan_Status) %>% 
  pivot_longer(c(1:3), names_to = "cat", values_to = "income") 

# plot

p1 <- ggplot(df6.3, aes(x = Loan_Status, y = income, shape = Loan_Status, colour = Loan_Status)) +
  geom_jitter(alpha = 0.5) + 
  geom_boxplot(alpha = 0,
               outlier.shape = NA,
               color = "black") +
  facet_wrap(~cat) +
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "none") +
  labs(x = "Loan Status",
       title = "Income Category vs Loan Status")

p2 <- ggplot(df6.3, aes(x = Loan_Status, y = log(income), shape = Loan_Status, colour = Loan_Status)) +
  geom_jitter(alpha = 0.5) + 
  geom_boxplot(alpha = 0,
               outlier.shape = NA,
               color = "black") +
  facet_wrap(~cat) +
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "none")+
  labs(x = "Loan Status",
       title = "Logged Income Category vs Loan Status",
       caption = "total income = applicant income + co-applicant income")

library(ggpubr)

ggarrange(p1, p2, 
          nrow = 2, 
          ncol = 1,
          labels =c("A", "B"))

```

My hypothesis is that the boxplots of "Y"s should be at a higher position than all the "N" because applicants in the "Y" had their application approved and therefore would have higher income.  

However, results show that regardless of whether the income is based on original scale or log transformed, 

* All three income data have no obvious impact on loan approval.   
* Boxplots of 3 type of incomes "ApplicantIncome", "CoapplicantIncome" and "total_income" overlap with each other.  

It indicates that the criteria of loan approval is not based on income, at least it is not obvious.  


### 6.4 Number of incomers

This section studies the effect of the number of incomers toward the outcome of a loan application. 

The "Applicant" in the graph means that only primary applicant has income in an application, whereas, "Both" means both primary applicant and the co-applicant have incomes in an application. 

* From disapproved application (N), there is not much different between "Applicant" and "Both".

* From approved application (Y), "Both" has more application got approved (39% more).   

It is a good sign that feature engineering does help in extracting hidden trend in the data. 

```{r}

# set up data frame

df6.4 <- train.data %>% 
  dplyr::select(Income_provider, Loan_Status) %>% 
  group_by(Income_provider, Loan_Status) %>% 
  summarise(count = n()) %>% 
  mutate(lab = paste0(Income_provider, "\n(n =", count, ")"))

# plot

ggplot(df6.4, aes(x = lab, y = count, fill = Income_provider)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = count), vjust = 1.5) +
  facet_wrap(~ Loan_Status, scale = "free_x") +
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "top",
        axis.title.x = element_text(margin = margin(8, 0, 0, 0)),
        axis.title.y = element_text(margin = margin(0, 8, 0, 0))) +
  labs(x = "Loan Status",
       y = "Applicant Count",
       title = "Dual-incomers Help Loan Approval Positively (Y)")

```
In the approved application "Y", there are 38.95% more application in the "Both" group got their application approved. 

```{r}
(239-172)/172 *100

```

### 6.5 Marrital Status, Depedents and Education 

This section studies the effect of marital status, numbers of dependents, and education levels of an applicant on his/her loan status in the train.data dataset.

Results show that:

* There is no obvious trend for the number of dependents on loan approval.   
* 8% more applicants with graduate education level had their loan application approved.   
* 9% more applicants with a married status had their loan approved.   


```{r, fig.width=9, fig.height=7}
# set up df

df6.5 <- train.data %>% 
  dplyr::select(Married, Dependents, Education, Loan_Status) %>% 
  pivot_longer(c(1:3), names_to = "variables", values_to = "values") %>% 
  group_by(Loan_Status, variables, values) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  group_by(variables, values) %>% 
  mutate(total = sum(count),
         per = paste0(round(count/total,2)*100, "%"),
         lab = paste0(values, "\n", "n = (", total, ")"))

# plot

ggplot(df6.5, aes(x = lab, y = count, fill = Loan_Status)) +
  geom_histogram(stat = "identity", position = "fill") +
  facet_wrap(~variables, scale = "free") +  
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "top",
        axis.title.x = element_text(margin = margin(8, 0, 0, 0)))+
  labs(x = "Loan Status",
       y = " ",
       title = "Dependents, Education, and Marrital Status on Loan Status") +
  geom_label(aes(label = per), 
             position = "fill", 
             vjust = 2) 

```


### 6.6 Credit_History, Property_Area, Self_Employed

Insights show that:

* If an applicant does not have credit history, 90% of the chance that his/her application would be rejected. Credit history will help tremendous in getting loan application approved.

* There are 3 different types of property area, semi-urban has the highest approval rate (77%), followed by urban at 68%, and lastly semi-urban at 62%. 

* Self-employment would not have relation with loan approval.

```{r, fig.width=9, fig.height=7}
# Set up dataframe

df6.6 <- train.data %>% 
  dplyr::select(Self_Employed, Credit_History, Property_Area, Loan_Status) %>% 
  pivot_longer(c(1:3), names_to = "variables", values_to = "values") %>% 
  group_by(Loan_Status, variables, values) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  group_by(variables, values) %>% 
  mutate(total = sum(count),
         per = paste0(round(count/total,2)*100, "%"),
         lab = paste0(values, "\n", "n = (", total, ")"))

# plot

ggplot(df6.6, aes(x = lab, y = count, fill = Loan_Status)) +
  geom_histogram(stat = "identity", position = "fill") +
  facet_wrap(~variables, scale = "free") +  
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "top",
        axis.title.x = element_text(margin = margin(8, 0, 0, 0)))+
  labs(x = "Loan Status",
       y = " ",
       title = "Self-Employed, Credit History, and Property Area on Loan Status") +
  geom_label(aes(label = per), 
             position = "fill", 
             vjust = 1.5) 

```


### 6.7 EDA summary

From the exploratory data analysis, there are some visual trends in education status, marital status and property area that they may impact the approval of a loan application. Credit history had the largest impact on loan application outcome.  

Apart from the above visualisation analysis, Statistical modeling (machine learning) will be carried out next to help extract inner trends in the data and ultimately build an effective model to predict application outcome of the imported test.data dataset.


## 7 MACHINE LEARNING

### 7.1 Data partitioning

There are two datasets imported, the **train.data** and the **test.data**. However, only the **train.data** dataset has the responding variable **"Loan_Status"**. Therefore, my strategies will be (a common machine learning technique):

* Split the train.data dataset into proportioned **train.set** and **test.set**. This **test.set** will be used to assess models built from the **train.set**.

* The best model will be used to predict the **test.data**.

There are several ratio options for data partitioning, such as 50:50, 60:40, or 80:20 to split the data into train and test set. Most popular ratios are 40:60, 30:70, and 20:80, and usually the 30:70 is popular because it is in the "middle" position. 

However, data partitioning is largely depends on how much data we have. If dataset is sufficiently big, results yield from different partitioning ratio may be similar. In small dataset such as the dataset in this project, We should try our best to ensure there is enough data to train an effective model as well as enough data for the test set to assess the performance of the model. 

I am going for 70:30 split of the train.data into train.set and test.set to ensure there is sufficient data for both of the datasets.

```{r}
set.seed(123)

# Data partitioning

ind <- train.data$Loan_Status %>% createDataPartition(list = F, p = 0.7)

# train test split

train.set <- train.data[ind, ]
test.set <- train.data[-ind, ]

```

The train.data is a small dataset consisting only 592 rows of observation, and data partition splits it into:

* 70% train.set with 415 rows of observation and,    
* 30% of test.set with 177 rows of observation.  

Data size of **train.set**: 

```{r}
dim(train.set)[1]

```
Data size of **test.set**: 

```{r}
dim(test.set)[1]

```
The test set has 54 and 123 of "N" and "Y". I have a concern that the sample size for "N" might be insufficient and would cause low specificity. If it is the case I would need to try out other partitions such as 60:40 instead of current 30:70. Anyway, it will be detected in later stage.

```{r}

table(test.set$Loan_Status)

```


### 7.2 Handling Data Imbalance

As discussed, this dataset has class imbalance problem. Approximately 30% of the data in the responding variable is made up of "N", and 70% for "Y". It means that there is less data for models to learn to classify "N". 

Accuracy rate calculated in later section would be misleading because it could be dominated by "Y". Technically, it means a high level in accuracy rate, a high level in sensitivity but a low level in specificity. This project put equal weight on sensitivity and specificity, and therefore a model with similar accuracy rate, sensitivity and specificity is desired. 

```{r}
127/(127+288)*100

```

To solve this data unbalance problem, there are 4 techniques:

1. *Over-sampling*: Increase the sample size of "N" by randomly repeat some of the existing data.   
2. *Under-sampling*: Reduce the sample size of "Y" to the sample size of "N". Some data of "Y" will be lost.   
3. *Both-sampling*: It uses over-sampling and under-sampling to a dataset and make a balanced size between both "Y" and "N" group.     
4. *ROSE*: Create any desired sample size by synthesising artificial data. However, some misleading data would need to be handled. For example, the minimum of age is 0, but it could be a negative value if ROSE is applied. Therefore, Data needs to be checked and cleaned if ROSE is applied.    


```{r}
set.seed(123)

over_train <- ovun.sample(Loan_Status~., data = train.set, method = "over", N = 288*2)$data  # 288 is the major group in the binary
under_train <- ovun.sample(Loan_Status~., data = train.set, method = "under", N = 127*2)$data  # it reduces "Y"'s 288 samples to 127.
both_train <- ovun.sample(Loan_Status ~., data = train.set, method = "both", N = 415)$data # total samples of train set
rose_train <- ROSE(Loan_Status ~., data = train.set, N = 500)$data

```

From my experience, the method "both" (combination of over-sampling and under-sampling) generally worked  well, and this technique will be used in this project.

```{r}
table(both_train$Loan_Status)

```

* The "N" had 127 observations, and it has been increased to 208 by re-sampling. In re-sampling, it increases the sample size by randomly repeating some of the existing information.
* The "Y" had 288 observations, and it has been reduced to 207 with some information lost.  
* The ratio of N:Y has now been improved from approximately 30:70 to approximately 50:50.


### 7.3 About Modeling 

It should be aware that no model is 100% accurate in machine learning. Generally, different models from different appropriate algorithms are built and the best model is searched and used to predict an unknown dataset. 


### 7.4 Logistic Regression

This project faces a binary / binomial problem which means the responding variable has only 2 group, such as yes or no. Logistic regression will be applied. For logistic regression, it is not required to test the linear relationship between predictors and the responding variable, and therefore the normality of error terms.

Following show the results of a binomial logistic regression model with a 10-fold cross validation (cv) and a cv repetition of 3 times. 

```{r, warning=FALSE}
model_logistic <- train(Loan_Status ~ ., 
                        data = both_train,
                        method = "glm",
                        family = "binomial",
                        trControl = trainControl(method = "repeatedcv",
                                                 number = 10,
                                                 repeats = 3))

summary(model_logistic)

```

**Insights**

* Results show that variable **total_income** and the level "480" in the variable **Loan_Amount_per_term** are redundant because all other variable have provided sufficient information in explaining the variability in the responding variables. 

* The model shows that most variables are not significantly related to the loan status (which is the outcome variable), except for **gender (male), marital Status (yes), dependents (2), co-applicant Income, credit history (1 & not sure) and property area (semi-urban)**. These are statistically important variables that is said to have real effect (Will not have 0 effect, but can be negative) on the approval of loan, though they are statistically significant, but their effects are subjected to their coefficient estimates (log odd) and one's significant effect sometime might be too small to have a decisive impact.    

A new logistic model is built with only significant variables as below. 

```{r}
model_logistic <- train(Loan_Status ~ Gender + Married + Dependents + CoapplicantIncome + Credit_History + Property_Area, 
                        data = both_train,
                        method = "glm",
                        family = "binomial",
                        trControl = trainControl(method = "repeatedcv",
                                                 number = 10,
                                                 repeats = 3))

summary(model_logistic)
  
```

**Receiver Operating Characteristics Curve (ROC)**

A ROC curve is plotted to search for the optimum AUC (area under the curve) that would give rise to the best sensitivity, specificity and the probability cutoff point. 

```{r}

# Prediction on test.data

pred_logistic <- model_logistic %>% predict(test.set, type = 'prob') 

# ROC curve

roc_logistic <- roc(test.set$Loan_Status,   
               pred_logistic[, 1])      # first column is "Y"        

plot.roc(roc_logistic, 
         print.auc = T, 
         print.thres = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Logistic")


```
**Insights**

* The higher the AUC, the better the model. The diagonal line is where AUC is equal to 0.5, it is where the binary classification is based on random chance.    
* AUC values of 0.756 is considered a good because it is higher than 0.5.  
* The best probability cutoff point suggested by this ROC with this AUC is 0.380. It will be a probability cutoff point that any observation above this point will be classified as "Y". 

**Prediction with test data**

Start prediction based on the probability cutoff point 0.380. 

```{r}

# Set up data frame

CM_logistic <- pred_logistic %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.38 = ifelse(Y > 0.38, "Y", "N"),
         result_based0.38 = as.factor(result_based0.38))

# Confusion Matrix

confusionMatrix(CM_logistic$result_based0.38,
                test.set$Loan_Status,
                positive = "Y")

```

This model has a very good accuracy rate at 82.5% but in above case, this accuracy rate is very miss-leading because it can only correctly predicting "Y" at 97.6% (sensitivity) and when predicting "N", the accuracy drops to only 48% (specificity).

Let's try out the default 0.5 cut-off point, it is the default classification cut-off point if ROC is not generated for the best suggested probability cutoff point. The default 0.5 generally works well but ROC's probability cutoff point is generally recommended. 

```{r}
pred_logistic <- model_logistic %>% predict(test.set, type = 'raw') 


confusionMatrix(pred_logistic,
                test.set$Loan_Status,
                positive = "Y")
```
The default probability cutoff point of 0.5 is a better point to be used in this case. 

Though the accuracy rate drops from 82.5% to 73.4%, but the gap between sensitivity and specificity become much smaller. In other words, the accuracy rate of correctly predicting "N" increased at the trade-off of sensitivity. The overall accuracy rate become less miss-leading. 

The accuracy rate is also higher than the "No Information Rate" of 69%, but it's 95% CI has cover the no information rate. It indicates that there is a chance that this model can be useless. "No information rate" is the chance of getting "Y" when the classification is made based on random chance. 

One may say the probability cut-off point of 0.38 is better because it has higher rate for accuracy and sensitivity, however, whether a model is good or bad is largely depends on the purpose of the prediction. If predicting "Y" is more important in a project than predicting "N" then the model with probability point of 0.38 is better than the 0.5 cutoff point.

However, there is no specific purpose in this project specifying which sensitivity or specificity need to be emphasized and so I will be choosing the model that has somehow equal sensitivity and specificity (or closer gap).  

Therefore, in logistic regression, 0.5 cutoff point is much better. 


### 7.5 Tree Models

4 type of tree models will be built in this section, which are:

* Decision Tree  
* Bootstrap Aggregating (Bagging)
* Random Forest 
* Extreme Gradient Boosting


#### 7.5.1 Decision Tree Model

A decision tree model is built. It is a popular non-parametric algorithm that can handle missing data (though I have handled them), and is immune to multicollinearity and outliers. It is a method usually built for quick understanding of the variables and their effects on the outcome.

Decision tree continuously splits the data into 2 and repeatedly until maximum homogeneity within the new parts are achieved. The entire structure is like a upside-down tree with most important variables being located at the top, and there are decision rules in each split to help making classification. 
 
**Model building**

```{r}

set.seed(123)

model_dt <- train(Loan_Status ~., 
                  data = both_train,
                  method = "rpart",
                  trControl = trainControl(method = 'repeatedcv', 
                                           number = 10, 
                                           repeats = 3))
model_dt

```

A decision tree model is built and 0.0338 is suggested the best CP. CP is a value to limit the size of a decision tree and the default is 0.01. The default CP level generally can do a good job however the optimum one is generally generated and will be automatically selected by the model. 

From following graph, CP at 0.0338 has the highest accuracy. 

```{r}
plot(model_dt)

```

This decision tree model has following decision rules:

```{r}
model_dt$finalModel

```
The "*" means that branch is a terminal node. 

Following is a better, as well a typical representation of a decision tree. The root node with the most important variable is located at the top, and each split will create two branches and two root nodes. Nodes at the very based are terminal node where the splitting stops. During each split, decision roles will be made available.  

The tree helps us to make decision. For examples, if an observation has credit history of 0, it will be directed to "No" at the root node, and it will be classified as disapproved case "N" by 82% of chance. This concept is the same to the left direction where the result is leading to "Y". 

```{r}

rattle::fancyRpartPlot(model_dt$finalModel)

```
**Receiver Operating Characteristics Curve (ROC)**

Similar to logistic regression, ROC is also applied to find its suggested probability cutoff point to be used for prediction. 

```{r}

# prediction on test set

pred_dt <- model_dt %>% predict(test.set, type = "prob")


# ROC curve

roc_dt <- roc(test.set$Loan_Status, 
              pred_dt[, 1])       # Picking "Y"

plot.roc(roc_dt,
         print.auc = T,
         print.thres = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Decision Tree")


```
The best probability cutoff point suggested by ROC for decision tree model to predict "Y" is 0.255. Any observation above 0.255 will be classified as "Y". 

**Predictions**

This prediction will be based on the ROC suggested probability cutoff point 0.255 to determine if an observation is "Y".

```{r}

CM_dt <- pred_dt %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.745 = ifelse(Y > 0.255, "Y", "N"),
         result_based0.745 = as.factor(result_based0.745))

# Confusion matrix

confusionMatrix(CM_dt$result_based0.745, test.set$Loan_Status, positive = "Y")

```
The model has fairy high accuracy rate at 77.4% with a rate higher than "No Information Rate", it indicates that the model is useful. However, the probability gives miss-leading accuracy rate, because there is a big gap between sensitivity and specificity. There is at least 37% different between sensitivity and specificity.

```{r}
 0.8862 - 0.51

```

Let's try the default 0.5 cutoff point. 

```{r, warning=FALSE, message=FALSE}

CM_dt2 <- model_dt %>% predict(test.set, type = "raw")

confusionMatrix(CM_dt2, test.set$Loan_Status, positive = "Y")

```
With the probability cutoff point of 0.5, the gap between sensitivity and specificity become smaller with a difference at 20.4%. However, it is still considered a big gap. 

```{r}
0.7967-0.5926
```

I will take this decision tree as a not-so-good model for prediction. 


#### 7.5.2 Bagging model

Now, I am building more than just a single tree by using the "bagging" method.    

The *bagging* stands for "Bootstrap Aggregating". This method samples the training set multiple times, randomly with replacement and build multiple trees. The decision of any classification will be based on the overall result of all the trees. 

**Model building**

Building the model: 

```{r}
set.seed(123)

model_bag <- train(Loan_Status ~., 
                   data = both_train,
                   method = "treebag",
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10, 
                                            repeats = 3,
                                            allowParallel = T))
model_bag

```

There were 25 bootstrap replications involved in this algorithm. 

```{r}
model_bag$finalModel

```

**Receiver Operating Characteristics Curve (ROC)**

This is to find the best Probability cut off point to be used for prediction. 

```{r}
# prediction

predict_bag <- model_bag %>% predict(test.set, type = "prob")

# ROC curve

roc_bag <- roc(test.set$Loan_Status,
               predict_bag[, 1])

plot.roc(roc_bag,
         print.thres = T,
         print.auc = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Bagging")


```
The best probability cutoff point is suggested to be 0.660. Any observation with probability above this level will be classified as "Y" by the bagging model. 

```{r}
# set up

CM_bag <- predict_bag %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.3 = ifelse(Y > 0.66, "Y", "N"),
         result_based0.3 = as.factor(result_based0.3))

# Confusion Metrics

confusionMatrix(CM_bag[, 2], 
                test.set$Loan_Status, 
                positive = "Y")

```

**Insights**

* The model has a good accuracy rate of 72.3% and this is higher than the "No information rate" of 69.5%. It indicates that this model is useful and perform better than random chance.  

* There is a different between sensitivity and specificity of 13%. It is acceptable if there is no other model perform better than this value. 

```{r}
0.81-0.68
```
Even the default probability cutoff point of 0.5 has a larger different between sensitivity and specificity rate at 17% (shown below). Therefore, for my purpose of searching for a fair, accurate model which put the same weight on both sensitivity and specificity, the probability point of 0.66 suggested by ROC is doing a better job. 

```{r}

# set up

CM_bag2 <- model_bag %>% predict(test.set, type = "raw")

# Confusion Metrics

confusionMatrix(CM_bag2, 
                test.set$Loan_Status, 
                positive = "Y")

```
```{r}
0.8049-0.6296
```
Again, there is no right or wrong about a model. It depends on the purpose of the prediction. If the purpose is to predict Y only, then this model with 0.5 probability cutoff point is certainly a better model than the model that used the R0C recommended cutoff point at 0.66.


#### 7.5.3 Random Forest

Random forest is one of the powerful machine learning algorithm that many machine learning practitioners prefer to use. 

It is similar to bagging method that build many decisions trees based on random re-sampling with replacement. 500 tree models will be built in this random forest model. 500 is the default value but is change-able. The only difference between random forest and bagging is that random forest limit each tree in the selection of available variables (or known as predictors in machine learning). 


```{r}

set.seed(123)

model_rf <- train(Loan_Status~., 
                  data = both_train,
                  method = "rf",
                  trControl = trainControl(method = "repeatedcv",
                                           number = 10,
                                           repeats = 3, 
                                           allowParallel = T))
model_rf

```
The random forest model says that:

* It is a classification task
* 500 trees were built 
* It suggests that the optimum number of variables (mtry) at each split is 26 and will be used when doing prediction. 

```{r}
model_rf$finalModel

```
The accuracy is optimum when "mtry" is equal to 26.

```{r}
plot(model_rf)

```

**Receiver Operating Characteristics Curve (ROC)** 

ROC is plotted to find the best probability cutoff point to classify whether an application is "Y" or "N".

```{r}
# prediction
pred_rf <- model_rf %>% predict(test.set, type = "prob")

# ROC curve

ROC_rf <- roc(test.set$Loan_Status,
    pred_rf[, 1])

plot.roc(ROC_rf,
         print.thres = T,
         print.auc = T, 
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Random Forest")

```

The best probability cutoff points is 0.523, with AUC (area under the curve) equal to 0.802 which is a value higher than the 50% random chance. Therefore, it is a useful model that can perform better than random chance. 

```{r}
# Set df

CM_rf <- pred_rf %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.523 = ifelse(Y > 0.523, "Y", "N"),
         result_based0.523 = factor(result_based0.523))

# Confusion matrix

confusionMatrix(CM_rf[, 2], test.set$Loan_Status, positive = "Y")

```

Random forest is performing superior than other models built previously. 

* This random forest model has the highest accuracy at 77.4%.    
* This accuracy is higher than the "No Information Rate" at 69.5%.  
* The "No Information Rate" is outside the 95% CI of 70.5%-83.3%, it indicates that the accuracy is significant.    
* The sensitivity is the highest so far at 80.5%.    
* The specificity is not the highest but is fairly good at 70.37%.  
* The gap between sensitivity and specificity is the smallest compared to other models built.  

Let's see the performance of default probability cutoff point 0.5 for random forest model. 

```{r}
CM_rf2 <- model_rf %>% predict(test.set, type = "raw")

confusionMatrix(CM_rf2, test.set$Loan_Status, positive = "Y")



```
Random forest model with 0.5 probability cutoff point do no have a better performance. The accuracy is lower, the gap between sensitivity and specificity is larger, and "No Information Rate" fall into the 95% CI range which indicating that the model may not perform better than random chance when predicting "Y".

Therefore, random forest model is the best model so far with the ROC suggested probability cutoff point at 0.523.


#### 7.5.4 XgbTree

Extreme gradient boosting (XgbTree) is being run in this section. It is an extremely popular method, it is similar to random forest but the difference is that there is no bootstrap sampling and the tree models are being built sequentially. It means trees are not built parallee, each tree built will learn the mistake from the previous tree. Therefore, the trees built will become better and better. 

Building the model with complex tunning parameter:

```{r}
#set.seed(123)

#model_xgb <- train(Loan_Status ~.,
#                   data = both_train,
#                   method = "xgbTree",
#                   trControl = trainControl(method = "repeatedcv",
#                                            number = 10,
#                                            repeats = 3),
#                   tuneGrid = expand.grid(nrounds = 500,
#                                          eta = c(0.01, 0.05),
#                                          max_depth = c(2, 4, 6),
#                                          colsample_bytree = c(0.5, 1),
#                                          subsample = c(0.50, 1),
#                                          gamma = 0.5,
#                                          min_child_weight = c(0, 20)))

```

Save and load the model:

```{r}
# Save the model 
#saveRDS(model_xgb, "model_xgboosting.rds")

# load the model
model_xgb <- readRDS("model_xgboosting.rds")

```


**Receiver Operating Characteristics Curve (ROC)** 

ROC is plotted to find the best probability cutoff point to classify whether an application is "Y" or "N".

```{r}
pred_xgb <- model_xgb %>% predict(test.set, type = "prob")

roc_xgb <- roc(test.set$Loan_Status, 
               pred_xgb[, 1])

plot.roc(roc_xgb,
         print.thres = T,
         print.auc = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Extreme Gradient Boosting")

```
The optimum probability cutoff point suggested by ROC is 0.372. 

**Prediction**

```{r}
CM_df <- pred_xgb %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.372 = ifelse(Y > 0.372, "Y", "N"),
         result_based0.372 = as.factor(result_based0.372))

confusionMatrix(CM_df[,2], test.set$Loan_Status, positive = "Y")

```

Accuracy rate based on the recommended probability 0.372 is very misleading because the gap between sensitivity and specificity is too high, at 30.98%.

```{r}
90.24-59.26

```
For example, if I say this model has 80.8% accuracy but when predicting "N", its actual accuracy is only 59.26% (defined as specificity). 

When the default 0.5 probability cutoff point is used, model performance become better. 

```{r}
CM_df2 <- model_xgb %>% predict(test.set, type = "raw")

confusionMatrix(CM_df2, test.set$Loan_Status, positive = "Y")

```
However, its accuracy rate is lower than the random forest model at 77.4% built in previous section. This xgboost model might also be useless by chance as the 95% CI range cover the "No Information Rate" of 69.5%. The gap between sensitivity and specificity is also higher when compared to the random forest model.

Therefore, random forest is the best algorithm among all model built previously and will be used to predict the unknown dataset, **test.data** that do not have the outcome variable "Loan_Status". 


### 7.6 Variable Importance

Before making the prediction, following are Importance plots by the tree models I have built. It is the special type of plot by tree algorithms to help us tell which variables are important.

Since, the random forest model is the best model, it should be given more weight. The random forest model suggests that credit history, applicant income, total income (applicant income + co-applicant income), loan amount per term (loan amount per month), and the loan amount are the top 5 most important variables.

This result is supported by other tree-based algorithms, though the ranking of the top 5 or top variables might be slightly different but they tell the same story. Slight fluctuation is expected. Again, the result of random forest model should be emphasized as this is the best model. 

```{r, fig.width=10, fig.height=10}

p1 <- plot(varImp(model_dt), main = "Decision Tree")
p2 <- plot(varImp(model_bag), main = "Bagging")
p3 <- plot(varImp(model_rf), main = "Random Forest")
p4 <- plot(varImp(model_xgb), main = "XgBoosting")

ggarrange(p1, p2, p3, p4,
          ncol = 2,
          nrow = 2)

```


### 7.7 Final Prediction

After building various machine learning models, trained them using **train.set**, and evaluated them using **test.set**。 Random forest model with the probability cutoff point at 0.523 had the best performance, and therefore this model will be selected to make the final prediction.

The final prediction is carried out on the **test.data** dataset. This dataset do not have loan_status to show application outcome. Therefore, it is a dataset without results and requiring the best model to predict. 

This model has 362 applicants (also can be known as observation or rows) and 14 variables. 

```{r}
dim(test.data)
```

**Removing the level “Co-applicant”**

Before making prediction, the level “Co-applicant" in the variable "Income_provider" will be removed. 

This level means whether the application has only the co-applicant has income, and the main applicant does not has an income. There are only 2 observation (data). All the models built, especially the best random forest model did not consider this level and therefore these 2 observation will be removed before making prediction.

```{r}
summary(test.data$Income_provider)
```
Removing the level:

```{r}
test.data2 <- test.data %>% 
  filter(Income_provider != 'Co-applicant')

```

**Removing the levels in the Loan Amount Term**

Similar concept, the levels "6" (1 application) and "350" (1 application) in the variable "Loan Amount Term" will be removed. They cannot be predicted based on the built random forest model and so they will be removed. 

The levels used to build the model: 

```{r}
summary(test.set$Loan_Amount_Term)
```
The levels in the unknown dataset:

```{r}
summary(test.data$Loan_Amount_Term)
```

Removing the levels:

```{r}

test.data2 <- test.data2 %>% 
  filter(Loan_Amount_Term != 6,
         Loan_Amount_Term != 350)
  
summary(test.data2$Loan_Amount_Term)

```
Making the final prediction on the unknown dataset:

```{r}
# Final Prediction with Random forest model at 0.523 probability cutoff point

final_prediction <- model_rf %>% predict(test.data2, type = "prob")

final_prediction <- final_prediction %>% 
  dplyr::select(Y) %>% 
  mutate(Kar_prediction = ifelse(Y > 0.523, "Y", "N"),
         Kar_prediction = as.factor(Kar_prediction)) %>% 
  rename(probability = Y)

# set up df

df <- test.data2 %>% 
  mutate(Kar_prediction = final_prediction$Kar_prediction) %>% 
  dplyr::select(Kar_prediction) %>% 
  group_by(Kar_prediction) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(total = sum(count),
         per = round(count/total, 2))

# plot

ggplot(df, aes(x = "", y = count, fill = Kar_prediction)) +
  geom_bar(stat = "identity", colour = "black") +
  coord_polar(theta = "y", start = 0) +
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_label(aes(label = paste0(Kar_prediction, "\n", count, " (", per*100, " %)")),
             position = position_stack(vjust = 0.5),
             size = 4) +
  labs(title = "Results of Final Prediction",
       subtitle = "By Random Forest Model (0.523 Probability cutoff point)") +
  scale_fill_manual(values = c("red", "green2"))




```

## 8 CONCLUSION

In conclusion, 

From EDA:

* There are more male applicants than female but both genders have equal acceptance-rejection rate at 70%:30%.  
* Tere is no clear trend that applicant income, co-applicant income and total income impact the success rate of loan.   
* However, if both applicant and the co-applicant of an application have incomes, it may help the application get an approved. In the approved application "Y", there are 38% more application with "dual-incomers" got their application approved.    
* Education may help a little, only 8% higher success rate.   
* 9% higher success rate for married applicants.   
* Applicants without credit history is highly likely get a rejection (only 8% succeed). Among all the approved application, 80% of applicants had credit history.   

From machine learning:

Logistic regression, decision tree, bagging, random forest, and tuned extreme gradient boosting models were built in this project. Random forest model with the probability cutoff point of Y at 0.523 had the best performing metrics. Compared to other model:  

* The model has the highest accuracy at 77.4%.    
* This accuracy is higher than the "No Information Rate" at 69.5%.  
* The "No Information Rate" is outside the 95% CI range of 70.5%-83.3%, it indicates that the accuracy rate is significant.    
* The sensitivity is the highest at 80.5%.    
* The specificity is not the highest but is fairly good at 70.37%.  
* The gap between sensitivity and specificity is the smallest compared to other models built.  

The random forest model suggests that following are the top 5 most important variables.    

* credit history,   
* applicant income,    
* total income (applicant income + co-applicant income)    
* loan amount per term (loan amount per month)     
* loan amount

The final prediction was made on a unknown dataset with 362 applications (observation) and predicted that 37% (133) of the applications will be rejected and 63%  (255) of the applications will be approved. 


*Thank you for reading.*


##  9 REFERENCE

Chatterjee D 2019, Loan Prediction Problem, viewed 24 January 2022, https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset 
