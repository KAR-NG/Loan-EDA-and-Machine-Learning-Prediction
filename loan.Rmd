---
title: "Loan EDA and Machine Learning Prediction"
author: "Kar Ng"
date: "1/24/2022"
output: 
  github_document: 
    toc: true
    toc_depth: 4
always_allow_html: yes
---

***


***


## 1 SUMMARY





## 2 R PACKAGES

```{r}
library(tidyverse)
library(skimr)
library(caret)
library(kableExtra)
library(glmnet)
library(MASS)
library(pROC)
library(rpart)
library(rpart.plot)
library(rattle)   # plot tree from caret
library(ggrepel)
library(ROSE)

```

## 3 INTRODUCTION

It is a machine learning project to demonstrate my technical experiences. This project uses datasets that are related to loan borrowing. 

I will use statistical modeling in machine learning to study the datasets and to extract important variables that are related to loan approval by the loan provider. I will also create predictive models that are able to make accurate prediction when new data are coming.

Information to be analysed include gender, marital status, number of dependents, education level, self employment, applicant income, co-applicant income, loan amount, loan amount term, credit history, and area of respondents' property. 

Two datasets are given, one is named 'train.data' and the another named 'test.data'. Both datasets have same information but the train.data has one more variable 'loan status', which contains the loan results.

There will be 3 datasets involved in this project. During modeling, I will split the train.data into two sbuject, one is a smaller subset with a name **"train_set"** and another named **"test_set"**. The **train_set** will be used to make various models and the **test_set** will be used to evaluate these models.

Ultimately, the best model will be used to predict the 'test.data'. 



## 4 DATA PREPARATION

Data is downloaded from the [Kaggle](https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset). Two datasets are downloaded, a train and a test data.

### 4.1 Data import

The train and the test data are imported in following code chunk.

```{r}
train.data <- read.csv("train_loan.csv", na.strings = c("", "NA"))
test.data <- read.csv("test_loan.csv", na.strings = c("", "NA"))

```

### 4.2 Data exploration

There are 13 variables for the train.data and 12 variables for the test.data. Both dataset have similar variable, however there is an additional variable in train.data, the "Loan_Status" which records the loan outcome. Test.data is the dataset that requiring me to make prediction using the train.data. 

Following show the first 6 rows of the train.data and the test.data.

```{r}
head(train.data)

```

```{r}
head(test.data)
     
```
From structural analysis, 

* There are 614 observations (rows) in the train.data dataset and 367 observations in the test.data dataset.      
* Train.data has the last variable 'Loan_status' that has either 'Y' or 'N' to indicate the success of a loan.  
* The 'Loan_Amount_Term', which is the length of time a loan is to be completely paid off, has repeated levels. For example, the replication of 360. It may indicate that this variable has categorical nature and should be converted in factor type. 

```{r}
str(train.data)

```

```{r}
str(test.data)

```

To detect missing values (NA), 

* In the train.data,  
  * There are 8 character and 5 numerical variables.
  * The dataset is quite complete (checking on the **n_missing** and **complete_rate**).   
  * There are many variables with a few missing values, and they have complete_rate higher 90%.  
  * The variable **Credit_History** is either 1 or 0, which is a binary variable and should be converted into factor during data cleaning. 
  
```{r}
skim_without_charts(train.data)

```
* The same situation goes to the test.data dataset. 

```{r}
skim_without_charts(test.data)

```
## 5 DATA CLEANING AND MANIPULATION

### 5.1 Train.data 

#### 5.1.1 Remove ID and factor conversion

First, I will remove the "Loan_ID" and convert all character variables into factor because they are categorical variable. I remove LOan_ID because it is meaningless for predictive analysis. 

Converting variables into factors will help my analysis because it gives the data a grouping features which will enable useful R function as well as visualisation in later stage. 

```{r}

train.data <- train.data %>% 
  dplyr::select(-Loan_ID) %>%                                    # Remove Loan_ID 
  mutate_if(is_character, as.factor) %>%                  # Convert all character into factor.
  mutate(Credit_History = as.factor(Credit_History),      # Convert Credit_History into factor.
         Loan_Amount_Term = as.factor(Loan_Amount_Term))  # Convert Loan_Amount_Term into factor.

```

After converting categorical variables into factor, following function help to summarise the dataset.  

```{r}

summary(train.data)

```
Many variables have missing values, as denoted by "NA's". Missing values will be filled up in next section.


#### 5.1.2 Replacing NA

Following codes replace all the NA with respective mode, which is the most occurring category of variable they belong. It is a decision after carefully examining its feasibility, these missing values are less than 5%, and their respective most occurring category have frequencies that are way higher than the rest of the categories. 

```{r}
train.data <- train.data %>% 
  mutate(Gender = replace_na(Gender, "Male"),
         Married = replace_na(Married, "Yes"),
         Dependents = replace_na(Dependents, '0'),
         Self_Employed = replace_na(Self_Employed, 'No'),
         Loan_Amount_Term = replace_na(Loan_Amount_Term, '360'))
  
summary(train.data)

```

**Credit History**

Regarding the NA in credit history, I found that the 50 NA is too many (8%), and is quite close to 89 of "0". I decided to make it a new level. 

```{r}
50/(89+475+50) 

```
```{r}
train.data <- train.data %>% 
  mutate(Credit_History = as.character(Credit_History),
         Credit_History = replace_na(Credit_History, "Not_sure"),
         Credit_History = as.factor(Credit_History))
```


**LoanAmount**

There are 22 missing values out from 614 rows of data, which is only 3.5%. I will remove these missing values. According to Schafer (1999), the paper asserted that a missing rate of 5% or less is inconsequential.

```{r}
22/614 * 100
```
```{r}
train.data <- train.data %>% na.omit()

```

### 5.2 Test data 

#### 5.1.1 Remove ID and factor conversion

Same process applies to test data. 

```{r}

test.data <- test.data %>% 
  dplyr::select(-Loan_ID) %>%                                    # Remove Loan_ID 
  mutate_if(is_character, as.factor) %>%                  # Convert all character into factor.
  mutate(Credit_History = as.factor(Credit_History),      # Convert Credit_History into factor.
         Loan_Amount_Term = as.factor(Loan_Amount_Term))  # Convert Loan_Amount_Term into factor.

summary(test.data)

```
#### 5.1.2 Replacing NA

```{r}
test.data <- test.data %>% 
  mutate(Gender = replace_na(Gender, "Male"),
         Dependents = replace_na(Dependents, '0'),
         Self_Employed = replace_na(Self_Employed, 'No'),
         Loan_Amount_Term = replace_na(Loan_Amount_Term, '360'))
  
summary(test.data)

```

**Credit History**

For credit history, I will transfer 29 of "NA" to "1", because it is very likely that all these 29 NA are actually 1  because 1 is the most occuring category of that variable.  

```{r}
test.data <- test.data %>% 
  mutate(Credit_History = as.character(Credit_History),
         Credit_History = replace_na(Credit_History, "1"),
         Credit_History = as.factor(Credit_History))

summary(test.data)

```

**Loan Amount**

There are only 5 NA in the variable, I will remove these missing values. 

```{r}
test.data <- test.data %>% 
  na.omit()

```

Both datasets have now been cleaned and are ready for analysis. 


### 5.3 Feature Engineering

**On Train.data**

This section creates 3 new variables based on original variables. 

1. total_income (Applicant Income + Co-applicant income)

This is synthesised by adding "ApplicantIncome" and "CoapplicantIncome" to find the total income of an applicant. 

```{r}
train.data <- train.data %>% 
  mutate(total_income = ApplicantIncome + CoapplicantIncome) %>% 
  relocate(total_income, .after = CoapplicantIncome)

```

2. Loan_Amt_per_term (Loan/month)

This is synthesised by dividing "LoanAmount" with "Loan_Amount_Term" to, interestingly, find out would the loan amount with the unit of per month affect the approval of a loan application. 

```{r}
train.data <- train.data %>% 
  mutate(Loan_Amt_per_term = round(LoanAmount/as.numeric(Loan_Amount_Term), 2)) %>% 
  relocate(Loan_Amt_per_term, .after = Loan_Amount_Term)

```


3. Income provider (one incomer or two)

This variable is created based on how many incomers in an application. Two levels have been detected for this new variable:

* "Applicant": Only the primary applicant has income

* "Both": Both the primary applicant and co-applicant have incomes

```{r}
train.data <- train.data %>% 
  mutate(Income_provider = case_when(CoapplicantIncome == 0 ~ "Applicant",
                                     ApplicantIncome == 0 ~ "Co-applicant",
                                     TRUE ~ "Both"),
         Income_provider = as.factor(Income_provider)) %>% 
  relocate(Income_provider, .after = total_income)

```

**On Test.data**

The same operation applies to test.data.


1. total_income

```{r}
test.data <- test.data %>% 
  mutate(total_income = ApplicantIncome + CoapplicantIncome) %>% 
  relocate(total_income, .after = CoapplicantIncome)

```

2. Loan_Amt_per_term

```{r}
test.data <- test.data %>% 
  mutate(Loan_Amt_per_term = round(LoanAmount/as.numeric(Loan_Amount_Term), 2)) %>% 
  relocate(Loan_Amt_per_term, .after = Loan_Amount_Term)

```


3. Income provider (one incomer or two)

```{r}
test.data <- test.data %>% 
  mutate(Income_provider = case_when(CoapplicantIncome == 0 ~ "Applicant",
                                     ApplicantIncome == 0 ~ "Co-applicant",
                                     TRUE ~ "Both"),
         Income_provider = as.factor(Income_provider)) %>% 
  relocate(Income_provider, .after = total_income)

```


## 6 EXPLORATORY DATA ANALYSIS (EDA)

In this section, I will use the **train.data** dataset to analyse general trends in the dataset because **train.data** has "Loan_Status" which is the responding variable. The relationship between Loan_Status with various variables in the dataset will be studied.

```{r}
names(train.data)

```
This exploratory data analysis will serve as a tool to understand the general trends underneath the data before getting into machine learning and statistical modeling.

### 6.1 Overall Approval Rate 

Most of the applications were approved from the train dataset were approved. 411 applications (70%) were approved, denoted with the symbol "Y"， whereas 181 of applications (30%) were rejected, denoted by "N".

The loan status is the responding variable of this analysis and therefore the unbalanced sample size from both "Y" and "N" may badly affect the predictive model (sensitivity and specificity) built in the later machine learning section. It is a good note here and will be considered during models building and evaluation.

```{r, message=FALSE, warning=FALSE}

# set up dataframe

df6.1 <- train.data %>% 
  dplyr::select(Loan_Status) %>% 
  group_by(Loan_Status) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(total = sum(count),
         per = round(count/total, 1))

# plot

ggplot(df6.1, aes(x = "", y = count, fill = Loan_Status)) +
  geom_bar(stat = 'identity', colour = "black") +
  coord_polar(theta = "y", start = 0) +
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_label(aes(label = paste0(Loan_Status, "\n", count, " (", per*100, " %)")),
             position = position_stack(vjust = 0.5),
             size = 4) +
  labs(title = "70% Of Applications have been Approved",
       subtitle = "(Train data)")  

```

### 6.2 Would Gender affect Loan Approval

It will be interesting to see the number of female and male applicants and would gender affect loan approval. Data shows that:

* There were lesser female applicants (109) than male applicants (483).

* However, the loan success rate of both gender are the same, both arrived at 70%. There is no detection of gender bias from the dataset. 

```{r, fig.width = 8, message=FALSE}

# set up dataframe

df6.2 <- train.data %>% 
  dplyr::select(Loan_Status, Gender) %>% 
  group_by(Loan_Status, Gender) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  group_by(Gender) %>% 
  mutate(total = sum(count),
         per = round(count/total, 1))

# plot

ggplot(df6.2, aes(x = "", y = count, fill = Loan_Status)) +
  geom_bar(stat = 'identity', colour = "black") +
  coord_polar(theta = "y", start = 0) +
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_label(aes(label = paste0(Loan_Status, "\n", count, " (", per*100, " %)")),
             position = position_stack(vjust = 0.5),
             size = 3) +
  labs(title = "Both Gender have similar proportion",
       subtitle = "(Train data)") +
  facet_wrap(~ Gender)


```
Female Applicants:

```{r}
73+36
```
Male Applicants:

```{r}
145+338

```

### 6.3 Income and Loan Approval

There are three type of income categories to be analysed, which are:

* "ApplicantIncome" - the income of the primary applicant  
* "CoapplicantIncome" - the income of co-applicant  
* "total_income" - the total income of both primary applicant and co-applicant  

```{r, fig.width=8, fig.height=7}

# set up dataframe

df6.3 <- train.data %>% 
  dplyr::select(ApplicantIncome, CoapplicantIncome, total_income, Loan_Status) %>% 
  pivot_longer(c(1:3), names_to = "cat", values_to = "income") 

# plot

p1 <- ggplot(df6.3, aes(x = Loan_Status, y = income, shape = Loan_Status, colour = Loan_Status)) +
  geom_jitter(alpha = 0.5) + 
  geom_boxplot(alpha = 0,
               outlier.shape = NA,
               color = "black") +
  facet_wrap(~cat) +
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "none") +
  labs(x = "Loan Status",
       title = "Income Category vs Loan Status")

p2 <- ggplot(df6.3, aes(x = Loan_Status, y = log(income), shape = Loan_Status, colour = Loan_Status)) +
  geom_jitter(alpha = 0.5) + 
  geom_boxplot(alpha = 0,
               outlier.shape = NA,
               color = "black") +
  facet_wrap(~cat) +
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "none")+
  labs(x = "Loan Status",
       title = "Logged Income Category vs Loan Status")

library(ggpubr)

ggarrange(p1, p2, 
          nrow = 2, 
          ncol = 1,
          labels =c("A", "B"))

```

I hypothesise that whoever have higher income, will have application approved, therefore, the boxplot of "Y" should be at a higher position than "N". 

Interestingly, results show that regardless of whether the income is based on original scale or log transformed, 

* all three income data has no obvious impact on loan approval.   
* Boxplots of 3 type of incomes "ApplicantIncome", "CoapplicantIncome" and "total_income" overlap with each other near perfectly. "Note: total_income is the combination of income from applicant and co-applicant."  

It indicates that the criteria of loan approval is not merely based on income. 


### 6.4 Number of incomers

This section studies the effect of the number of incomers toward the outcome of a loan application. 

The "Applicant" in the graph means that only primary applicant has income in an application, whereas, "Both" means both primary applicant and the co-applicant have incomes in an application. 

* From disapproved application (N), there is not much different between "Applicant" and "Both".

* From approved application (Y), more applications with dual-income, "Both" got approved.  

It is a good sign that feature engineering does help in extracting hidden trend in the data. It would be tested statistically in later section using machine learning technique. 

```{r}

# set up data frame

df6.4 <- train.data %>% 
  dplyr::select(Income_provider, Loan_Status) %>% 
  group_by(Income_provider, Loan_Status) %>% 
  summarise(count = n()) %>% 
  mutate(lab = paste0(Income_provider, "\n(n =", count, ")"))

# plot

ggplot(df6.4, aes(x = lab, y = count, fill = Income_provider)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = count), vjust = 1.5) +
  facet_wrap(~ Loan_Status, scale = "free_x") +
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "top",
        axis.title.x = element_text(margin = margin(8, 0, 0, 0)),
        axis.title.y = element_text(margin = margin(0, 8, 0, 0))) +
  labs(x = "Loan Status",
       y = "Applicant Count",
       title = "Dual-incomers Help Loan Approval Positively (Y)")

```
In the approved application "Y", there are 38% more application with "dual-incomers" got their application approved.  

```{r}
(239-172)/172 *100

```




### 6.5 Marrital Status, Depedents and Education 

This section studies the effect of marital status, numbers of dependents, and education levels of an applicant on his/her loan status in the train.data dataset.

Results show that:

* There is no obvious trend for the number of dependents on loan approval.   
* 8% more applicants with graduate education level had their loan application approved.   
* 9% more applicants with a married status had their loan approved.   


```{r, fig.width=9, fig.height=7}
# set up df

df6.5 <- train.data %>% 
  dplyr::select(Married, Dependents, Education, Loan_Status) %>% 
  pivot_longer(c(1:3), names_to = "variables", values_to = "values") %>% 
  group_by(Loan_Status, variables, values) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  group_by(variables, values) %>% 
  mutate(total = sum(count),
         per = paste0(round(count/total,2)*100, "%"),
         lab = paste0(values, "\n", "n = (", total, ")"))

# plot

ggplot(df6.5, aes(x = lab, y = count, fill = Loan_Status)) +
  geom_histogram(stat = "identity", position = "fill") +
  facet_wrap(~variables, scale = "free") +  
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "top",
        axis.title.x = element_text(margin = margin(8, 0, 0, 0)))+
  labs(x = "Loan Status",
       y = " ",
       title = "Dependents, Education, and Marrital Status on Loan Status") +
  geom_label(aes(label = per), 
             position = "fill", 
             vjust = 2) 

```


### 6.6 Credit_History, Property_Area, Self_Employed

Insights show that:

* If an applicant does not have credit history, 90% of the chance that his/her application would be rejected. Credit history will help tremendous in getting loan application approved.

* There are 3 different types of property area, semiurban has the highest approval rate (77%), followed by urban at 68%, and lastly semi-urban at 62%. 

* Self-employment would not affect the success rate of loan approval.

```{r, fig.width=9, fig.height=7}
# Set up dataframe

df6.6 <- train.data %>% 
  dplyr::select(Self_Employed, Credit_History, Property_Area, Loan_Status) %>% 
  pivot_longer(c(1:3), names_to = "variables", values_to = "values") %>% 
  group_by(Loan_Status, variables, values) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  group_by(variables, values) %>% 
  mutate(total = sum(count),
         per = paste0(round(count/total,2)*100, "%"),
         lab = paste0(values, "\n", "n = (", total, ")"))

# plot

ggplot(df6.6, aes(x = lab, y = count, fill = Loan_Status)) +
  geom_histogram(stat = "identity", position = "fill") +
  facet_wrap(~variables, scale = "free") +  
  theme_bw() +
  theme(strip.text = element_text(size = 14),
        plot.title = element_text(face = "bold"),
        legend.position = "top",
        axis.title.x = element_text(margin = margin(8, 0, 0, 0)))+
  labs(x = "Loan Status",
       y = " ",
       title = "Self-Employed, Credit History, and Property Area on Loan Status") +
  geom_label(aes(label = per), 
             position = "fill", 
             vjust = 1.5) 

```


### 6.7 EDA summary

From the exploratory data analysis, there are some visual trends in education status, marital status and property area that they may impact the approval of a loan application. The most obvious limiting factor is whether the applicant has previous credit history. 

Apart from the above visualisation analysis, Statistical modeling (machine learning) will be carried out to help extract the inner trends in the data and ultimately build an effective model to predict the imported test.data dataset.


## 7 MACHINE LEARNING

### 7.1 Data partitioning

There are two datasets imported, the **train.data** and the **test.data**. However, only the **train.data** datasets have the responding variable, **Loan_Status**. Therefore, my strategies will be:

* Split the train.data dataset into proportioned **train.set** and **test.set**. This test.set will be used to assessing models built from the **train.set**.

* Lastly, the best model will be used to predict the **test.data**.

There are several ratio options for data partitioning, such as 50:50, 60:40, or 80:20 split of the data into train and test set. Most popular ranges are 40:60, 30:70, and 20:80, and usually one can pick the 30:70 as it is in the "median" position. 

However, data partitioning is largely depends on how much data we have. We should try our best to ensure there is enough data to train an effective model as well as enough data for the test set to assess the performance of the model. 

I am going for 70:30 split of the train.data into train and test set to ensure there is sufficient enough for the test set for model evaluation purpose. 

```{r}
set.seed(123)

# Data partitioning

ind <- train.data$Loan_Status %>% createDataPartition(list = F, p = 0.7)

# train test split

train.set <- train.data[ind, ]
test.set <- train.data[-ind, ]

```

The train.data is a small dataset consisting only 592 rows of observation, and data partition splits it into 70% of train.set with 415 rows of observation (~70%) and 177 rows of observation (~30%).

Data size of **train.set**: 

```{r}
dim(train.set)[1]

```
Data size of **test.set**: 

```{r}
dim(test.set)[1]

```
The test set has 54 and 123 of "N" and "Y". There would be a little concern for "N" as its data size is not very large and therefore specificity of the model built would be affected. 


```{r}

table(train.set$Loan_Status)

```
### 7.2 Handling Data Imbalance

As discussed, this dataset has class imbalance problem. Approximately 30% of the data in the responding variable is made up of "N", and 70% for "Y". It means that there is less data for models to learn to classify "N". 

Accuracy rate calculated in later section would be misleading because it can be dominated by corrected predicted Y。Technically, it can be high in accuracy rate, high in sensitivity but low in specificity. 

```{r}
127/(127+288)*100
```
To solve this problem, I will use technique that can help making both "N" and "Y" having similar amount of data. This can be done using over-sampling, under-sampling, both-sampling, and synthetic data. Both-sampling is the technique that applies both over- and under-sampling. 


```{r}
set.seed(123)

over_train <- ovun.sample(Loan_Status~., data = train.set, method = "over", N = 288*2)$data  # 288 is the major group in the binary
under_train <- ovun.sample(Loan_Status~., data = train.set, method = "under", N = 127*2)$data  # it reduces "Y"'s 288 samples to 127.
both_train <- ovun.sample(Loan_Status ~., data = train.set, method = "both", N = 415)$data # total samples of train set
rose_train <- ROSE(Loan_Status ~., data = train.set, N = 500)$data

```

From my experience, the method "both" (combination of over-sampling and under-sampling) worked very well, and this will be used in this project. 
```{r}
table(both_train$Loan_Status)

```
* The "N" had 127 observations, and it has been increased to 208 by re-sampling. In re-sampling, it increases the sample size by randomly repeating some of the original information.
* The "Y" had 288 observations, and it has been reduced to 207 with some lost of information.  
* The ratio of N:Y has now been improved from approximately 30:70 to approximately 50:50.


### 7.3 About Modeling 

It should be aware that no model is 100% accurate in machine learning. Therefore, different models are generally built based on different algorithms to search for the model that can do the best job in prediction. 


### 7.4 Logistic Regression

This project faces a binary / binomial problem which means the responding variable has only 2 values, which is either yes or no. Logistic regression will be applied. Logistic regression can also be used for multinomial situation where responding variable has more than 2 levels. 

For logistic regression, it is not required to test the linear relationship between predictors and the responding variable, and therefore the normality of error terms.

Following show the results of a binomial logistic regression model with a 10-fold cross validation (cv) and a cv repetition of 3 times. 

```{r, warning=FALSE}
model_logistic <- train(Loan_Status ~ ., 
                        data = both_train,
                        method = "glm",
                        family = "binomial",
                        trControl = trainControl(method = "repeatedcv",
                                                 number = 10,
                                                 repeats = 3))

summary(model_logistic)

```

*Insights*

* Results show that variable **total_income** and the level "480" in the variable **Loan_Amount_per_term** are redundant because all other variable have provided sufficient information at 100% level in explaining the variability in the responding variables. 

* The model shows that most variables are not significantly related to the loan status (which is the responding variable), except for **gender (male), marital Status (yes), dependents (2), co-applicant Income, credit history (1 & not sure) and property area (semi-urban)**. These can be described as important variables that is said to have real effect (Will not have 0 effect, but can be negative) on the approval of loan, though they are statistically significant, but their effects are subjected to their coefficient estimates (log odd) and one's significant effect sometime might be too small to have a decisive impact.    

Therefore, a new logistic model is built with only significant variables as below. 

```{r}
model_logistic <- train(Loan_Status ~ Gender + Married + Dependents + CoapplicantIncome + Credit_History + Property_Area, 
                        data = both_train,
                        method = "glm",
                        family = "binomial",
                        trControl = trainControl(method = "repeatedcv",
                                                 number = 10,
                                                 repeats = 3))

summary(model_logistic)
  
```

**Receiver Operating Characteristics Curve (ROC)**

A ROC curve is plotted to search for the optimum AUC (area under the curve), sensitivity, specificity and the probability cutoff point. 

```{r}

# Prediction on test.data

pred_logistic <- model_logistic %>% predict(test.set, type = 'prob') 

# ROC curve

roc_logistic <- roc(test.set$Loan_Status,   
               pred_logistic[, 1])      # first column is "Y"        

plot.roc(roc_logistic, 
         print.auc = T, 
         print.thres = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Logistic")


```
Insights:

* The higher the AUC, the better the model. The diagonal line is where AUC equal to 0.5, where the binary classification is random.   
* AUC values of 0.756 is considered a good model and our model is very useful with a AUC of 0.620.
* The best probability cutoff point is 0.380. It will be a probability cutoff point that any obversation above this point will be classified as "Y". 

**Prediction with test data**

Start prediction based on the probability cutoff point 0.356 detected above. 

```{r}

# Set up dataframe

CM_logistic <- pred_logistic %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.38 = ifelse(Y > 0.38, "Y", "N"),
         result_based0.38 = as.factor(result_based0.38))

# Confusion Matrix

confusionMatrix(CM_logistic$result_based0.38,
                test.set$Loan_Status,
                positive = "Y")

```

This model has accuracy of 82%, it may seems high but it is miss-leading because it can only correctly predicting "Y" correctly at 97% (sensitivity) but very low accuracy of predicting "N" at only 48% (specificity).

Let's try out the default 0.5 cut-off point, it is the default classificiation cut-off point if ROC is not generated for its suggestion of the best probability cutoff point. 

```{r}
pred_logistic <- model_logistic %>% predict(test.set, type = 'raw') 


confusionMatrix(pred_logistic,
                test.set$Loan_Status,
                positive = "Y")
```
Although the accuracy rate drops from 82% to 73%, but the gap between sensitivity and specificity become much smaller. In other words, the accuracy rate of correctly predicting "N" increased. Additionally, the accuracy rate is also higher than the "No Information Rate" of 69%, but it's 95% CI has cover the no information rate. It indicates that there is a chance that this model can be useless. "No information rate" is the chance of getting "Y" when the classification is made based on random chance. 

One may say the probability cut-off point of 0.38 is better because it has higher rate for accuracy and sensitivity, however, whether a model is good or bad is largely depends on its prediction purpose. If predicting "Y" is more important than predicting "N" then model with probability point of 0.38 is better than the 0.5 cutoff point.

However, there is no specific purpose in this project and so I will be choosing the model that has somehow equal sensitivity and specificity (or closer gap). 

Therefore, in logistic regression, 0.5 cutoff point is much better. 


### 7.5 Tree Models

4 type of tree models will be built in this section, which are:

* Decision Tree  
* Bootstrap Aggregating (Bagging)
* Random Forest 
* Extreme Gradient Boosting


#### 7.5.1 Decision Tree Model

A decision tree model is built. It is a popular non-parametric method that can handle missing data (though I have handled them) amd immune to multicollinearity and outliers. It is a method usually built for quick understanding of the variables and their effects on the outcome.

Decision tree continuously split the data into 2 and repeatedly until maximum homogeneity within the new parts are achieved. The entire structure is like a upside-down tree with most important variables being located at the top, and each split there are decision rules to help making classification. 
 
**Model building**

```{r}

set.seed(123)

model_dt <- train(Loan_Status ~., 
                  data = both_train,
                  method = "rpart",
                  trControl = trainControl(method = 'repeatedcv', 
                                           number = 10, 
                                           repeats = 3))
model_dt

```

A decision tree model is built and 0.0338 is suggested the best CP. CP is a value to limit the size of a decision tree and the default is 0.01, and this value generally can do a good job however the optimum one is generally generated and will be automatically used.

From following graph, CP at 0.0338 has the highest accuracy. 

```{r}
plot(model_dt)

```

The best model has following decision rules:

```{r}
model_dt$finalModel

```
The "*" means that branch is a terminal node. Following is a better, as well a typical representation of a decision tree. The tree helps us to make decision. For examples, if an observation has credit history of 0, it will be directed to "No", and it should be a disapproved case by 82% of chance. This concept is the same to the left direction where the result is leading to "Y" of approved-loan scenario. 

```{r}

rattle::fancyRpartPlot(model_dt$finalModel)

```
**Receiver Operating Characteristics Curve (ROC)**

This is to find the best Probability cutoff point to be used for prediction. 

```{r}

# prediction on test set

pred_dt <- model_dt %>% predict(test.set, type = "prob")


# ROC curve

roc_dt <- roc(test.set$Loan_Status, 
              pred_dt[, 1])       # Picking "Y"

plot.roc(roc_dt,
         print.auc = T,
         print.thres = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Decision Tree")


```
The best probability cutoff point suggested by ROC for decision tree model to predict "Y" is 0.255. Any observation above 0.255 will be classified as "Y". 

**Predictions**

This predictions will based on the calculated best probability cutoff point 0.255 to determine an observation is "Y".

```{r}

CM_dt <- pred_dt %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.745 = ifelse(Y > 0.255, "Y", "N"),
         result_based0.745 = as.factor(result_based0.745))

# Confusion matrix

confusionMatrix(CM_dt$result_based0.745, test.set$Loan_Status, positive = "Y")

```
The model has fairy high accuracy rate at 77.4% with a rate higher than "No Information Rate", it indicates that the model is useful. However, the probability gives miss-leading accuracy rate, with big gap in sensitivity and specificity. There is at least 37% different between sensitivity and specificity.

```{r}
 0.8862 - 0.51
```

Let's try the default 0.5 cutoff point. 

```{r}

CM_dt2 <- model_dt %>% predict(test.set, type = "raw")

confusionMatrix(CM_dt2, test.set$Loan_Status, positive = "Y")

```
With the probability cutoff point of 0.5, the gap between sensitivity and specificity become smaller with a difference at 20.4%. However, it is still considered a large disparity. 

```{r}
0.7967-0.5926
```

I will take this decision tree as a not-so-good model for prediction. 


#### 7.5.2 Bagging model

Now, I am building more than just a single tree like in the decision tree section. Every single tree 

A method is called *bagging* which stands for "Bootstrap Aggregating". This method randomly samples the training set multiple times (with replacement, means after taking sample for the first model, the data is put back to be sampled for second model). Ultimately, many decion tree models are built, and the results of these models will be combined to make the best prediction. 

**Model building**

The model is built and showing following feature. 

```{r}
set.seed(123)

model_bag <- train(Loan_Status ~., 
                   data = both_train,
                   method = "treebag",
                   trControl = trainControl(method = "repeatedcv",
                                            number = 10, 
                                            repeats = 3,
                                            allowParallel = T))
model_bag

```

There were 25 bootstrap replications involved in the model. 

```{r}
model_bag$finalModel

```

**Receiver Operating Characteristics Curve (ROC)**

This is to find the best Probability cut off point to be used for prediction. 

```{r}
# prediction

predict_bag <- model_bag %>% predict(test.set, type = "prob")

# ROC curve

roc_bag <- roc(test.set$Loan_Status,
               predict_bag[, 1])

plot.roc(roc_bag,
         print.thres = T,
         print.auc = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Bagging")


```
The best probability cutoff point is suggested to be 0.660. Any observation with probability above this level will be classified as "Y" by bagging model. 

```{r}
# set up

CM_bag <- predict_bag %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.3 = ifelse(Y > 0.66, "Y", "N"),
         result_based0.3 = as.factor(result_based0.3))

# Confusion Metrics

confusionMatrix(CM_bag[, 2], 
                test.set$Loan_Status, 
                positive = "Y")

```

**Insights**

* The model has a good accuracy rate of 72% and this is higher than the "No information rate" of 69.5%. It indicates that this model is useful and perform better than random chance.  

* There is a different between sensitivity and specificity of 13%. It is acceptable if there is no other model perform better than this value. 

```{r}
0.81-0.68
```
Even the default probability cutoff point of 0.5 has a larger different between sensitivity and specificity rate of 17% as follow. Therefore, for my purpose of having an fair accurate model which put the same weight on both sensitivity and specificity, the probability point of 0.66 suggested by ROC is doing a better job. 

```{r}

# set up

CM_bag2 <- model_bag %>% predict(test.set, type = "raw")

# Confusion Metrics

confusionMatrix(CM_bag2, 
                test.set$Loan_Status, 
                positive = "Y")

```
```{r}
0.8049-0.6296
```
Again, there is no right or wrong about a model in this case. It depends on the purpose of the prediction. If the purpose to predict Y only, then this model with 0.5 probability cutoff point is certainly a better model than the model that used the R0C recommended cutoff point at 0.66.


#### 7.5.3 Random Forest

Random forest is one of the powerful machine learning algorithm that many machine learning practitioners love to use. 

It is similar to planting many decisions. There are in fact many possible outcomes other than the outcome calculated in the decision tree section. It is because of randomisation nature of the algorithm and how I set.seed (A R code for randomisation) a model such as set.seed(123). If I set a different randomisation index, the result might be different. Therefore, I am building 500 trees in the random forest in this section to take possible result variations into account. 500 is a default value and is changeable.  

Random forest applies the technique of bagging which will samples the train dataset numerous times to build numerous tree, however each tree has limited in the selection of available variables (or known as predictors in machine learning).  


```{r}

set.seed(123)

model_rf <- train(Loan_Status~., 
                  data = both_train,
                  method = "rf",
                  trControl = trainControl(method = "repeatedcv",
                                           number = 10,
                                           repeats = 3, 
                                           allowParallel = T))
model_rf

```
The random forest model says that:

* It is a classification task
* 500 trees were built 
* It suggests that the optimum number of variables (mtry) at each split is 26 and will be used when doing prediction. 

```{r}
model_rf$finalModel

```
The accuracy is optimum when "mtry" equal to 26.


```{r}
plot(model_rf)

```

**Receiver Operating Characteristics Curve (ROC)** 

ROC is plotted to find the best probability cutoff point to classify whether an application is "Y" or "N".

```{r}
# prediction
pred_rf <- model_rf %>% predict(test.set, type = "prob")

# ROC curve

ROC_rf <- roc(test.set$Loan_Status,
    pred_rf[, 1])

plot.roc(ROC_rf,
         print.thres = T,
         print.auc = T, 
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Random Forest")

```

The best probability cut off points is 0.523, with AUC of 0.802, which is higher than the 50% of random chance. Therefore it is a model that is perform better than random chance. 

```{r}
# Set df

CM_rf <- pred_rf %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.523 = ifelse(Y > 0.523, "Y", "N"),
         result_based0.523 = factor(result_based0.523))

# Confusion matrix

confusionMatrix(CM_rf[, 2], test.set$Loan_Status, positive = "Y")

```

Random forest is performing superior than other models that I built previously. 

* This random forest model has the highest accuracy at 77.4%.    
* This accuracy is higher than the "No Information Rate" at 69.5%.  
* The "No Information Rate" is outside the 95% CI of 70.5%-83.3%, it indicates that the accuracy is significant.    
* The sensitivity is the highest so far at 80.5%.    
* The specificity is not the highest but it is fairly good at 70.37%.  
* The gap between sensitivity and specificity is the smallest compared to other models built.  

Let's see the performance of default probability cutoff point 0.5 for random forest model. 

```{r}
CM_rf2 <- model_rf %>% predict(test.set, type = "raw")

confusionMatrix(CM_rf2, test.set$Loan_Status, positive = "Y")



```
Random forest model with 0.5 probability cutoff point do no have a good performance. The accuracy is lower, the gap between sensitivity and specificity is larger, and "No Information Rate" fall into the 95% CI range, indicating that the model may not perform better than random chance when predicting "Y".

Therefore, random forest model is the best model so far with the probability cutoff point at 0.523.


#### 7.5.4 XgbTree

Extreme gradient boosting (XgbTree) is being run in this section. It is an extremely popular method that having the concept as random forest but the different is that there is no bootstrap sampling and the tree models are being built sequentially that learn the mistake from its previous model. Therefore, the trees built are become better and better. 


```{r}
#set.seed(123)

#model_xgb <- train(Loan_Status ~.,
#                   data = both_train,
#                   method = "xgbTree",
#                   trControl = trainControl(method = "repeatedcv",
#                                            number = 10,
#                                            repeats = 3),
#                   tuneGrid = expand.grid(nrounds = 500,
#                                          eta = c(0.01, 0.05),
#                                          max_depth = c(2, 4, 6),
#                                          colsample_bytree = c(0.5, 1),
#                                          subsample = c(0.50, 1),
#                                          gamma = 0.5,
#                                          min_child_weight = c(0, 20)))

```

Save and load the model:

```{r}
# Save the model 
#saveRDS(model_xgb, "model_xgboosting.rds")

# load the model
model_xgb <- readRDS("model_xgboosting.rds")

```


**Receiver Operating Characteristics Curve (ROC)** 

ROC is plotted to find the best probability cutoff point to classify whether an application is "Y" or "N".

```{r}
pred_xgb <- model_xgb %>% predict(test.set, type = "prob")

roc_xgb <- roc(test.set$Loan_Status, 
               pred_xgb[, 1])

plot.roc(roc_xgb,
         print.thres = T,
         print.auc = T,
         auc.polygon = T,
         auc.polygon.col = "green",
         max.auc.polygon = T,
         main = "ROC - Extreme Gradient Boosting")

```
The optimum probability cutoff point is 0.372. 

**Prediction**

```{r}
CM_df <- pred_xgb %>% 
  dplyr::select(Y) %>% 
  mutate(result_based0.372 = ifelse(Y > 0.372, "Y", "N"),
         result_based0.372 = as.factor(result_based0.372))

confusionMatrix(CM_df[,2], test.set$Loan_Status, positive = "Y")

```

Accuracy rate based on the recommended probability 0.372 is very misleading because the gap betweeon sensitivity and specificity is too high, at 30.98%.

```{r}
90.24-59.26
```
If I say this model has 80.8% accuracy but when predicting "N", its actual accuracy is only 59.26% (defined as specificity). If the case is to have a fair accuracy in predicting "Y" and "N", then this xgboost model based on the 0.372 probability cutoff point is not a good model. 

When the default 0.5 probability cutoff point is used, the model performance betcome better. 

```{r}
CM_df2 <- model_xgb %>% predict(test.set, type = "raw")

confusionMatrix(CM_df2, test.set$Loan_Status, positive = "Y")

```
However, its accuracy rate is lower than random forest at 77.4% and this xgboost model might be useless by chance as the 95% CI range cover the "No Information Rate" of 69.5%. The gap between sensitivity and specificity is also higher for xgboost compared to random forest model.

Therefore, random forest is the best algorithm in the models I have built previously and will be used to predict the unknown dataset, **test.data** that do not have the loan approval data "Loan_Status". 

Looking at the variables of test.data:

```{r}
names(test.data)
```

### 7.6 Variable Importance

Before making the prediction, following are the Importance plot by the tree models I have built. It is the special type of plot by tree algorithm to tell which variables are important.

Since, the random forest model is the best model, it will be given more weight. The random forest model suggests that Credit History, applicant income, total income (applicant income + co-applicant income). loan amount per term (loan amount per month) and the loan amount are the top 5 most important variables.

This results are supported by other tree-based algorithms as well, though the ranking of the top 5 or top variables might be slightly different but they tell the same story. Slight fluctuation is expected. Again, the result of random forest model should be emphasized as this is the best model in the case. 

```{r, fig.width=10, fig.height=10}

p1 <- plot(varImp(model_dt), main = "Decision Tree")
p2 <- plot(varImp(model_bag), main = "Bagging")
p3 <- plot(varImp(model_rf), main = "Random Forest")
p4 <- plot(varImp(model_xgb), main = "XgBoosting")

ggarrange(p1, p2, p3, p4,
          ncol = 2,
          nrow = 2)

```


### 7.7 Final Prediction

After building various machine learning models, train them using **train.set**, and evaluate them using **test.set**。 Random forest model with the probability cutoff point at 0.523 had the best performance, and therefore this model is used to make the final prediction.

The final prediction is carried out on the **test.data** dataset. This dataset do not have loan_status to show approve or disapprove of a loan application. Therefore, it is a dataset without results and requiring the best model to predict. 

This model has 362 applicants (also can be known as observation or rows) and 14 variables. 

```{r}
dim(test.data)
```

**Removing the level “Co-applicant”**

Before making prediction, the level “Co-applicant" in the variable "Income_provider" will be removed. 

This level means whether the application has only the co-applicant has income, and the main applicant does not has an income. There are only 2 observation (data). All the models built, especially the best random forest model did not consider this level and therefore these 2 observation will be removed before making prediction before making prediction. 

```{r}
summary(test.data$Income_provider)
```
Removing the level:

```{r}
test.data2 <- test.data %>% 
  filter(Income_provider != 'Co-applicant')

```

**Removing the levels in the Loan Amount Term**

It is because that I set Loan amount term to be factor variable, therefore there are some levels in the unknown dataset are excluded when building the random forest model such as 6 (1 application) and 350 (1 application). They cannot be predicted based on the built random forest model and so they will be removed. 

The levels used to build the model: 

```{r}
summary(test.set$Loan_Amount_Term)
```
The levels in the unknown dataset:

```{r}
summary(test.data$Loan_Amount_Term)
```

Removing the levels:

```{r}

test.data2 <- test.data2 %>% 
  filter(Loan_Amount_Term != 6,
         Loan_Amount_Term != 350)
  
summary(test.data2$Loan_Amount_Term)
```
Making the final prediction on the unknown dataset:

```{r}
# Final Prediction with Random forest model at 0.523 probability cutoff point

final_prediction <- model_rf %>% predict(test.data2, type = "prob")

final_prediction <- final_prediction %>% 
  dplyr::select(Y) %>% 
  mutate(Kar_prediction = ifelse(Y > 0.523, "Y", "N"),
         Kar_prediction = as.factor(Kar_prediction)) %>% 
  rename(probability = Y)


```

Above are the results predicted for each application, total of 358 applications (rows).

Adding the results to the test.data table and make a graph to see the results:

```{r}
# set up df

df <- test.data2 %>% 
  mutate(Kar_prediction = final_prediction$Kar_prediction) %>% 
  dplyr::select(Kar_prediction) %>% 
  group_by(Kar_prediction) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(total = sum(count),
         per = round(count/total, 2))

```

```{r}
# plot

ggplot(df, aes(x = "", y = count, fill = Kar_prediction)) +
  geom_bar(stat = "identity", colour = "black") +
  coord_polar(theta = "y", start = 0) +
  theme_minimal() +
  theme(axis.title = element_blank(),
        legend.position = "none",
        plot.title = element_text(hjust = 0.5,
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5)) +
  geom_label(aes(label = paste0(Kar_prediction, "\n", count, " (", per*100, " %)")),
             position = position_stack(vjust = 0.5),
             size = 4) +
  labs(title = "Results of Final Prediction",
       subtitle = "By Random Forest Model (0.523 Probability cutoff point)") +
  scale_fill_manual(values = c("red", "green2"))




```

## 8 CONCLUSION

In conclusion based on the dataset with loan status (responding variable):

From EDA:

* There are more male applicants than female but both genders have equal approval-reject rate at 70%:30%.

* From EDA, there is no clear trend that applicant income, coapplicant income and total income impact the success rate of loan. 

* However, if both applicant and the co-applicant of an application have incomes, it may help the application get an approved. In the approved application "Y", there are 38% more application with "dual-incomers" got their application approved.  

* Education may help a little, only 8% higher success rate. 

* 9% higher success rate for married applicants. 

* Applicants without credit history is highly likely get a rejection (only 8% succeeed). 80% of Applicants with a credit history got their application approved. 

From machine learning:


Logistic regression, decision tree, bagging, random forest, and tuned extreme gradient boosting models were built in this project. Random forest model with the probability cutoff point of Y at 0.523 had the best performing metrics. Compared to other model:  

* The model has the highest accuracy at 77.4%.    
* This accuracy is higher than the "No Information Rate" at 69.5%.  
* The "No Information Rate" is outside the 95% CI of 70.5%-83.3%, it indicates that the accuracy rate is significant.    
* The sensitivity is the highest so far at 80.5%.    
* The specificity is not the highest but it is fairly good at 70.37%.  
* The gap between sensitivity and specificity is the smallest compared to other models built.  

The random forest model suggests  
* Credit History,   
* applicant income,   
* total income (applicant income + co-applicant income)  
* loan amount per term (loan amount per month)   
* loan amount, are the top 5 most important variables.

The final prediction was made on a unknown dataset with 362 applications (observation) and predicted that 37% (133) of the applications will be rejected and 63%  (255) of the applications will be approved. 



##  9 REFERENCE

https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset
